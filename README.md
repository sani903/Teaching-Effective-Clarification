# Teaching Agents to Ask Effective Clarfication Questions

[Paper](link-to-paper): We present methods for training agents to generate effective clarification questions by identifying key, empirically-grounded properties.

## ğŸ›  Setup

This project uses the [OpenHands agent framework](https://github.com/All-Hands-AI/OpenHands). Follow the OpenHands documentation for setup instructions.

Ensure you have the necessary dependencies installed before running experiments.

## ğŸ“‚ Project Structure

- `evaluation/benchmarks/swe_bench/` â€“ Contains scripts for running evaluation.
- `evaluation/benchmarks/swe_bench/scripts/main_run_infer.sh` â€“ Main script to run inference experiments.
- `data/data.xlsx` â€“ Generated clarification questions by different models and their rewrites.
- `openhands/training/train.py` â€“ Training code for the clarification question generation models.

## ğŸš€ Running Experiments

To run inference experiments, use the following command:
```bash
./evaluation/benchmarks/swe_bench/scripts/main_run_infer.sh [model_config] [git-version] [agent] [eval_limit] [max_iter] [num_workers] [dataset] [dataset_split]
```

Example:
```bash
./evaluation/benchmarks/swe_bench/scripts/main_run_infer.sh llm.eval_gpt4_1106_preview HEAD CodeActAgent 300 30 1 princeton-nlp/SWE-bench_Lite test
```

Due to the dependence on OpenHands agentic framework, running the scripts smoothly might require pulling the latest updates from the OpenHands repository. To do this, set up OpenHands as an Upstream Remote.
```bash
git remote add upstream https://github.com/All-Hands-AI/OpenHands.git
git fetch upstream
git merge upstream/main
```

## ğŸ“ Training

Our reward pipeline uses multi-stage evaluation to train agents to generate high-quality clarification questions:

![Reward Pipeline](assets/reward_pipeline.png)

To train clarification question generation models:
```bash
python openhands/training/train.py [arguments]
```
See `openhands/training/train.py` for available training arguments and configurations.

## ğŸ“Š Results

We see that our trained model matches the performance of GPT-5 with 41% fewer questions.
<img src="assets/task_success_vs_questions.png" width="400">


## ğŸ“Š Data & Results

- `data/data.xlsx` â€“ Contains clarification questions generated by different models along with human rewrites

## ğŸ™ Acknowledgements

The experiments are conducted using the [OpenHands agent framework](https://github.com/All-Hands-AI/OpenHands).

## ğŸ“– Citation

If you use this work, please cite our paper:
```bibtex
@misc{yourpaper2025,
      title={Teaching Agents to Ask Effective Clarfication Questions}, 
      author={Your Name and Collaborators},
      year={2025},
      eprint={xxxx.xxxxx},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/xxxx.xxxxx},
}
```
